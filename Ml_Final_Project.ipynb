{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9236b93f-37f7-411f-86bf-71dd7cc0ae0e",
   "metadata": {},
   "source": [
    "Code: Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5511b2e1-4342-4375-998c-30e6d340994f",
   "metadata": {},
   "source": [
    "a) The network F(x, t, Î¸) computes the denoising score which is the conditional mean in the diffusion process by using the U-Net architecture. The ScoreNet class is a time-dependent score-based model that uses U-Net architecture. At self.embed the time t is encoded using Gaussian Fourier features which are then passed through a dense layer. The Gaussian Fourier Projection class turns the time t into a high dimensional embedding to capture time. The output is then transofrmed by a dense layer to produce a time embedding. The first convolutional block processes the input image x. The input image is passed through conv1 and the time embedding is projected to match the feature map size using a dense layer (dense1). These are added rogether, normalized using gnorm1, and activated using a switch activation function. The network then downsamples the feature maps through a series of convolutional layers (conv2, conv3, conv4). Each downsampling block consists of a convolutional layer with a stride of 2, a dense layer that projects the time embedding to match the current feature map size, group normalization, and the swish activation function. Once we reach the bottleneck, the network unsamples the feature maps through transposed convolutional layers. Each unsampling block consists of a transposed convolutional layer. The swish activation function used helps maintain a flow of gradiants during backpropogation. Time t has an affect on every stage of this network. The encoded time embedding is added to the feature maps after each convolutional layer. Adding the encoded time embedding to the feature maps makes sure that the model is aware of the current timestep, allowing it to make time-dependent adjustments to the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b00d4f3b-4420-4d1d-bdfb-70339b30d7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Defining a time-dependent score-based model (double click to expand or collapse)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class GaussianFourierProjection(nn.Module):\n",
    "  \"\"\"Gaussian random features for encoding time steps.\"\"\"\n",
    "  def __init__(self, embed_dim, scale=30.):\n",
    "    super().__init__()\n",
    "    # Randomly sample weights during initialization. These weights are fixed\n",
    "    # during optimization and are not trainable.\n",
    "    self.W = nn.Parameter(torch.randn(embed_dim // 2) * scale, requires_grad=False)\n",
    "  def forward(self, x):\n",
    "    x_proj = x * self.W * 2 * np.pi\n",
    "    return torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)\n",
    "\n",
    "\n",
    "class Dense(nn.Module):\n",
    "  \"\"\"A fully connected layer that reshapes outputs to feature maps.\"\"\"\n",
    "  def __init__(self, input_dim, output_dim):\n",
    "    super().__init__()\n",
    "    self.dense = nn.Linear(input_dim, output_dim)\n",
    "  def forward(self, x):\n",
    "    return self.dense(x)[..., None, None]\n",
    "\n",
    "\n",
    "class ScoreNet(nn.Module):\n",
    "  \"\"\"A time-dependent score-based model built upon U-Net architecture.\"\"\"\n",
    "\n",
    "  def __init__(self, channels=[32, 64, 128, 256], embed_dim=256, group_num=4):\n",
    "\n",
    "    super().__init__()\n",
    "\n",
    "    self.embed = nn.Sequential(GaussianFourierProjection(embed_dim=embed_dim),\n",
    "         nn.Linear(embed_dim, embed_dim))\n",
    "\n",
    "    self.conv1 = nn.Conv2d(1, channels[0], 3, stride=1, bias=False)\n",
    "    self.dense1 = Dense(embed_dim, channels[0])\n",
    "    self.gnorm1 = nn.GroupNorm(group_num, num_channels=channels[0])\n",
    "    self.conv2 = nn.Conv2d(channels[0], channels[1], 3, stride=2, bias=False)\n",
    "    self.dense2 = Dense(embed_dim, channels[1])\n",
    "    self.gnorm2 = nn.GroupNorm(group_num, num_channels=channels[1])\n",
    "    self.conv3 = nn.Conv2d(channels[1], channels[2], 3, stride=2, bias=False)\n",
    "    self.dense3 = Dense(embed_dim, channels[2])\n",
    "    self.gnorm3 = nn.GroupNorm(group_num, num_channels=channels[2])\n",
    "    self.conv4 = nn.Conv2d(channels[2], channels[3], 3, stride=2, bias=False)\n",
    "    self.dense4 = Dense(embed_dim, channels[3])\n",
    "    self.gnorm4 = nn.GroupNorm(group_num, num_channels=channels[3])\n",
    "\n",
    "    self.tconv4 = nn.ConvTranspose2d(channels[3], channels[2], 3, stride=2, bias=False)\n",
    "    self.dense5 = Dense(embed_dim, channels[2])\n",
    "    self.tgnorm4 = nn.GroupNorm(group_num, num_channels=channels[2])\n",
    "    self.tconv3 = nn.ConvTranspose2d(channels[2] + channels[2], channels[1], 3, stride=2, bias=False, output_padding=1)\n",
    "    self.dense6 = Dense(embed_dim, channels[1])\n",
    "    self.tgnorm3 = nn.GroupNorm(group_num, num_channels=channels[1])\n",
    "    self.tconv2 = nn.ConvTranspose2d(channels[1] + channels[1], channels[0], 3, stride=2, bias=False, output_padding=1)\n",
    "    self.dense7 = Dense(embed_dim, channels[0])\n",
    "    self.tgnorm2 = nn.GroupNorm(group_num, num_channels=channels[0])\n",
    "    self.tconv1 = nn.ConvTranspose2d(channels[0] + channels[0], 1, 3, stride=1)\n",
    "\n",
    "    # The swish activation function\n",
    "    self.act = lambda x: x * torch.sigmoid(x)\n",
    "    #added rho0 and rho1\n",
    "    self.rho_0 = nn.Parameter(torch.tensor(1.0))\n",
    "    self.rho_1 = nn.Parameter(torch.tensor(1.0))\n",
    "\n",
    "  def forward(self, x, t):\n",
    "    # Obtain the Gaussian random feature embedding for t\n",
    "    embed = self.act(self.embed(t))\n",
    "\n",
    "    h1 = self.conv1(x) # ...\n",
    "    h1 += self.dense1(embed) #...\n",
    "    h1 = self.gnorm1(h1) # ...\n",
    "    h1 = self.act(h1) # ...\n",
    "    h2 = self.conv2(h1) # ...\n",
    "    h2 += self.dense2(embed)\n",
    "    h2 = self.gnorm2(h2)\n",
    "    h2 = self.act(h2)\n",
    "    h3 = self.conv3(h2)\n",
    "    h3 += self.dense3(embed)\n",
    "    h3 = self.gnorm3(h3)\n",
    "    h3 = self.act(h3)\n",
    "    h4 = self.conv4(h3)\n",
    "    h4 += self.dense4(embed)\n",
    "    h4 = self.gnorm4(h4)\n",
    "    h4 = self.act(h4)\n",
    "\n",
    "    h = self.tconv4(h4) # ...\n",
    "    h += self.dense5(embed) # ...\n",
    "    h = self.tgnorm4(h)\n",
    "    h = self.act(h)\n",
    "    h = self.tconv3(torch.cat([h, h3], dim=1)) # ...\n",
    "    h += self.dense6(embed)\n",
    "    h = self.tgnorm3(h)\n",
    "    h = self.act(h)\n",
    "    h = self.tconv2(torch.cat([h, h2], dim=1))\n",
    "    h += self.dense7(embed)\n",
    "    h = self.tgnorm2(h)\n",
    "    h = self.act(h)\n",
    "    h = self.tconv1(torch.cat([h, h1], dim=1))\n",
    "\n",
    "    F_xt_theta = h\n",
    "\n",
    "    mu_xt_theta = self.rho_0 * (x - self.rho_1 * F_xt_theta)\n",
    "\n",
    "    return mu_xt_theta\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d4983bf-cb4d-4ef0-a498-a46122337f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Diffusion(nn.Module):\n",
    "    def __init__(self, model, n_steps, device, min_beta, max_beta):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.n_steps = n_steps\n",
    "        self.device = device\n",
    "        \n",
    "        #alpha = 1 - beta and alpha bar is product sum of alphas\n",
    "        self.beta = torch.linspace(min_beta, max_beta, n_steps).to(device)\n",
    "        self.alpha = 1 - self.beta\n",
    "        self.alpha_bar = torch.cumprod(self.alpha, dim = 0).to(device)\n",
    "        #store beta, alpha, \\bar alpha    \n",
    "    \n",
    "    def forward_process(self, x0, t):\n",
    "        \n",
    "        # finding xt given x0, sqrt of alpha bar times x0 + the sqrt of 1 - alpha bar epsilon\n",
    "        noise = torch.randn_like(x0).to(self.device)\n",
    "        alpha_bar_t = self.alpha_bar[t].reshape(-1, 1, 1, 1)\n",
    "        x_t = torch.sqrt(alpha_bar_t) * x0 + torch.sqrt(1 - alpha_bar_t)* noise\n",
    "        return x_t\n",
    "    #sample x_{t-1}, x_t, given x_0\n",
    "    \n",
    "    def predict_next(self, xt, t):\n",
    "        #use mu xt theta formula from number 8 in theory\n",
    "        # t_tensor = t / self.n_steps\n",
    "        # t_tensor = t_tensor.reshape(-1,1).to(self.device)\n",
    "\n",
    "        t_tensor = (t / self.n_steps).view(xt.shape[0], 1).to(self.device)\n",
    "        e_theta_xt = self.model(xt, t_tensor)\n",
    "        alpha_t = self.alpha[t].reshape(-1, 1, 1, 1)\n",
    "        alpha_bar_t = self.alpha_bar[t].reshape(-1, 1, 1, 1)\n",
    "        \n",
    "        mu_xt_theta = (1 / torch.sqrt(alpha_t)) * (xt - ((1 - alpha_t) / torch.sqrt(1 - alpha_bar_t)) * e_theta_xt)\n",
    "        return mu_xt_theta\n",
    "    \n",
    "    #compute mu(xt, t)\n",
    "    \n",
    "    \n",
    "    def sample_xt_xt_minus_1(self, x0, num_steps):\n",
    "        \n",
    "        #to sample xt minus 1 we take mu_xt_t_theta and add sqare root of 1 - alpha t times epsilon\n",
    "        \n",
    "        x_t = self.sample_xt(x0, t)\n",
    "        \n",
    "        mu_xt_theta = self.predict_next(x_t, t)\n",
    "        \n",
    "        noise = torch.randn_like(x_t).to(self.device)\n",
    "        \n",
    "        beta_t = self.beta[t].reshape(-1, 1, 1, 1)\n",
    "        \n",
    "        #use beta to scale in the reverse\n",
    "        x_t_minus_1 = mu_xt_theta + torch.sqrt(beta_t) * noise\n",
    "    \n",
    "        return x_t, x_t_minus_1\n",
    "    \n",
    "    def new_sample_xt_xt_minus_1(self, x0, t):\n",
    "        \n",
    "        x_t = self.forward_process(x0, t)\n",
    "        \n",
    "        \n",
    "        mu_xt_theta = self.predict_next(x_t, t)\n",
    "        \n",
    "        # use conditional mean of xt and x0\n",
    "        alpha_t = self.alpha[t]\n",
    "        alpha_bar_t = self.alpha_bar[t]\n",
    "        alpha_bar_t_minus_1 = self.alpha_bar[t-1] if t > 0 else torch.tensor(1.0, device = self.device)\n",
    "        mu_t_xt_x0 = ((1 - alpha_t) * torch.sqrt(alpha_bar_t_minus_1) * x0 + (1 - alpha_bar_t_minus_1) * torch.sqrt(alpha_t) * x_t) / (1 - alpha_bar_t)\n",
    "        #get noise based on xt\n",
    "        noise = torch.rand_like(x_t).to(self.device)\n",
    "        # variance\n",
    "        rho_t = torch.sqrt((1 - alpha_t) * (1 - alpha_bar_t_minus_1) / (1 - alpha_bar_t)).reshape(-1, 1, 1, 1)\n",
    "        #calculate xt-1 using conditional and variance and noise based on xt\n",
    "        x_t_minus_1 = mu_t_xt_x0 + rho_t * noise\n",
    "    \n",
    "        return x_t, x_t_minus_1\n",
    "    \n",
    "    \n",
    "    \n",
    "    def compute_loss(self, x0):\n",
    "        # get batch size from x0\n",
    "        batch_size = x0.shape[0]\n",
    "        # get random time step t so we can train on different time steps\n",
    "        t = torch.randint(0, self.n_steps, (batch_size,), device = self.device).long()\n",
    "        \n",
    "        x_t_list = []\n",
    "        x_t_minus_1_list = []\n",
    "        # for each batch we sample xt and xt-1 given x0 at the sampled time step\n",
    "        for i in range(batch_size):\n",
    "          x_t, x_t_minus_1 = self.new_sample_xt_xt_minus_1(x0[i:i+1], t[i])\n",
    "          x_t_list.append(x_t)\n",
    "          x_t_minus_1_list.append(x_t_minus_1)\n",
    "        \n",
    "        x_t = torch.cat(x_t_list, dim = 0)\n",
    "        x_t_minus_1 = torch.cat(x_t_minus_1_list, dim = 0)\n",
    "        # preduct the mean\n",
    "        mu_xt_theta = self.predict_next(x_t, t)\n",
    "        # use prediced mean to calculate loss and compare predicted mean of reverse with the actual sampled values\n",
    "        loss = ((x_t_minus_1 - mu_xt_theta) ** 2 / (2 * (1 - self.alpha[t].reshape(-1, 1, 1, 1)))).mean()\n",
    "        return loss\n",
    "    \n",
    "    def reverse(self, x_t, num_steps):\n",
    "        #iterate over time steps in reverse order, start from T-1 to 1\n",
    "        \n",
    "        x = x_t\n",
    "        for t in reversed(range(1, num_steps)):\n",
    "          batch_size = x_t.shape[0]\n",
    "          # use predicted mean, gives estimated denoised version of the current sample\n",
    "          t_tensor = torch.randint(0, self.n_steps, (batch_size,), device = self.device).long()\n",
    "          mu_xt_theta = self.predict_next(x, t = t_tensor)\n",
    "          # if t > 1 add gaussian noise to the sample, if t =1 then zero noise\n",
    "          noise = torch.randn_like(x).to(self.device) if t > 1 else torch.zeros_like(x).to(self.device)\n",
    "          # use beta to denoise sample\n",
    "          beta_t = self.beta[t].reshape(-1, 1, 1, 1)\n",
    "          #gets sample xt-1 using the predicted mean and scaling by Bt\n",
    "          x = mu_xt_theta + torch.sqrt(beta_t) * noise\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f988a684-8730-4126-b5cb-772d5928753f",
   "metadata": {},
   "source": [
    "e) To reduce the variance of the estimate, we can use the fact that q(x_{t-1} | x_t, x_0) is Gaussian. We can then use the conditional distribution to sample better. We will use rho_t to help reduce the variance of our loss estimates by using a more accurate mean for x_{t-1}. We first compute the conditional mean and then get the sample noise, we scale the noise by rho_t to reduce the variance of the estimate by calculating the uncertainty at each time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28c07939-d5de-4123-aa4d-1a6e662e78e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_mnist():\n",
    "    data = np.float64(np.load(\"MNIST_data.npy\"))\n",
    "    labels = np.float32(np.load(\"MNIST_labels.npy\"))\n",
    "    \n",
    "    data = data / 255\n",
    "    data.shape\n",
    "    labels.shape\n",
    "    \n",
    "    print(data.shape)\n",
    "    data=np.float32(data)/255.\n",
    "    train_data=data[0:50000].reshape((-1,1,28,28))\n",
    "    train_labels=np.int32(labels[0:50000])\n",
    "    val_data=data[50000:55000].reshape((-1,1,28,28))\n",
    "    val_labels=np.int32(labels[55000:60000])\n",
    "    test_data=data[55000:65000].reshape((-1,1,28,28))\n",
    "    test_labels=np.int32(labels[55000:65000])\n",
    "    \n",
    "    data_train = torch.tensor(train_data)\n",
    "    labels_train = torch.tensor(train_labels, dtype=torch.long)\n",
    "    data_val = torch.tensor(val_data)\n",
    "    labels_val = torch.tensor(val_labels, dtype=torch.long)\n",
    "    data_test = torch.tensor(test_data)\n",
    "    labels_test = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "    train_dataset = TensorDataset(data_train, labels_train)\n",
    "    val_dataset = TensorDataset(data_val, labels_val)\n",
    "    test_dataset = TensorDataset(data_test, labels_test)\n",
    "    batch_size = 100\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, test_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6be70527-4f8c-4269-a960-87c302c1c5ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n"
     ]
    }
   ],
   "source": [
    "def train_diffusion(model, train_loader, val_loader, num_epochs, device):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
    "    rho_optimizer = torch.optim.Adam([model.model.rho_0, model.model.rho_1], lr = 0.2)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch in train_loader:\n",
    "            x0, _ = batch\n",
    "            x0 = x0.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            rho_optimizer.zero_grad()\n",
    "            loss = model.compute_loss(x0)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            rho_optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /=len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                x0, _ = batch\n",
    "                x0 = x0.to(device)\n",
    "                loss = model.compute_loss(x0)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/ {num_epochs}, Train Loss: {train_loss: .4f}, Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "\n",
    "train_loader, val_loader, test_loader, test_dataset = get_mnist()\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "score_model = ScoreNet().to(device)\n",
    "diffusion_model = Diffusion(score_model, n_steps = 200, device = device, min_beta = 0.0001, max_beta = 0.1, tens = False).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "829ddb56-0f6e-4cff-85d2-08dfc83679a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/ 10, Train Loss:  0.0667, Val Loss: 0.0513\n",
      "Epoch 2/ 10, Train Loss:  0.0468, Val Loss: 0.0419\n",
      "Epoch 3/ 10, Train Loss:  0.0415, Val Loss: 0.0410\n",
      "Epoch 4/ 10, Train Loss:  0.0412, Val Loss: 0.0415\n",
      "Epoch 5/ 10, Train Loss:  0.0411, Val Loss: 0.0408\n",
      "Epoch 6/ 10, Train Loss:  0.0411, Val Loss: 0.0439\n",
      "Epoch 7/ 10, Train Loss:  0.0464, Val Loss: 0.0456\n",
      "Epoch 8/ 10, Train Loss:  0.0441, Val Loss: 0.0428\n",
      "Epoch 9/ 10, Train Loss:  0.0433, Val Loss: 0.0436\n",
      "Epoch 10/ 10, Train Loss:  0.0487, Val Loss: 0.0490\n"
     ]
    }
   ],
   "source": [
    "train_diffusion(diffusion_model, train_loader, val_loader, num_epochs = 10, device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e628324e-c1f1-4fdc-bab4-593049b654b6",
   "metadata": {},
   "source": [
    "(70000, 784)\n",
    "Epoch 1/ 60, Train Loss:  0.0651, Val Loss: 0.0507\n",
    "Epoch 2/ 60, Train Loss:  0.0464, Val Loss: 0.0443\n",
    "Epoch 3/ 60, Train Loss:  0.0416, Val Loss: 0.0410\n",
    "Epoch 4/ 60, Train Loss:  0.0412, Val Loss: 0.0409\n",
    "Epoch 5/ 60, Train Loss:  0.0411, Val Loss: 0.0409\n",
    "Epoch 6/ 60, Train Loss:  0.0411, Val Loss: 0.0410\n",
    "Epoch 7/ 60, Train Loss:  0.0443, Val Loss: 0.0431\n",
    "Epoch 8/ 60, Train Loss:  0.0459, Val Loss: 0.0439\n",
    "Epoch 9/ 60, Train Loss:  0.0429, Val Loss: 0.0437\n",
    "Epoch 10/ 60, Train Loss:  0.0434, Val Loss: 0.0409\n",
    "Epoch 11/ 60, Train Loss:  0.0421, Val Loss: 0.0411\n",
    "Epoch 12/ 60, Train Loss:  0.0410, Val Loss: 0.0409\n",
    "Epoch 13/ 60, Train Loss:  0.0409, Val Loss: 0.0411\n",
    "Epoch 14/ 60, Train Loss:  0.0411, Val Loss: 0.0407\n",
    "Epoch 15/ 60, Train Loss:  0.0409, Val Loss: 0.0407\n",
    "Epoch 16/ 60, Train Loss:  0.0408, Val Loss: 0.0409\n",
    "Epoch 17/ 60, Train Loss:  0.0408, Val Loss: 0.0406\n",
    "Epoch 18/ 60, Train Loss:  0.0408, Val Loss: 0.0407\n",
    "Epoch 19/ 60, Train Loss:  0.0408, Val Loss: 0.0406\n",
    "Epoch 20/ 60, Train Loss:  0.0407, Val Loss: 0.0405\n",
    "Had to stop after 20 because was taking long to run and want to continue testing, kernel died so switched epochs to 10 to be able to rerun all parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "129b8764-25e5-4827-864e-a5d321fc6305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images(model, num_samples, num_steps, device):\n",
    "    x_t = torch.randn(num_samples, 1, 28, 28).to(device)\n",
    "    samples = model.reverse(x_t = x_t, num_steps = num_steps)\n",
    "    return samples\n",
    "\n",
    "def show_samples(samples, n = 20):\n",
    "    samples = samples.cpu().detach().numpy()\n",
    "    fig, axes = plt.subplots(1, n, figsize = (n, 1))\n",
    "    for i in range(n):\n",
    "        axes[1].imshow(samples[i, 0], cmap = 'gray')\n",
    "        axes[i].axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0621a2c-995f-46bd-9461-e0587dafc555",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spicy as sp\n",
    "# from spicy.linalg import sqrtm\n",
    "\n",
    "def calculate_fid(real_activations, generated_activations):\n",
    "\n",
    "    activations_dtype = real_activations.dtype\n",
    "    if activations_dtype != np.float64:\n",
    "        real_activations = real_activations.astype(np.float64)\n",
    "        generated_activations = generated_activations.astype(np.float64)\n",
    "    \n",
    "    m = np.mean(real_activations, 0)\n",
    "    m_w = np.mean(generated_activations, 0)\n",
    "    num_examples_real = float(real_activations.shape[0])\n",
    "    num_examples_generated = float(generated_activations.shape[0])\n",
    "    \n",
    "    real_centered = real_activations - m\n",
    "    sigma = real_centered.T.dot(real_centered) / (num_examples_real - 1)\n",
    "    \n",
    "    gen_centered = generated_activations - m_w\n",
    "    sigma_w = gen_centered.T.dot(gen_centered) / (num_examples_generated - 1)\n",
    "    \n",
    "    sqrt_sigma = sp.linalg.sqrtm(sigma)\n",
    "    sqrts = sqrt_sigma.dot(sigma_v.dot(sigma))\n",
    "    \n",
    "    sqrt_trace_component = np.trace(sp.linalg.sqrtm(sigma))\n",
    "    \n",
    "    trace = np.trace(sigma + sigma_w) - 2.0 * sqrt_trace_component\n",
    "    \n",
    "    mean = np.sum(np.square(m - m_w))\n",
    "    fid = trace + mean\n",
    "    if activations_dtype != np.float64:\n",
    "        fid = fid.astype(activations_dtype)\n",
    "    \n",
    "    return fid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d29670-1974-436b-adcb-efa1cbfa148a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000\n",
    "\n",
    "score_model = ScoreNet().to(device)\n",
    "diffusion_model = Diffusion(score_model, n_steps = 200, device = device, min_beta = 0.0001, max_beta = 0.1).to(device)\n",
    "samples = sample_images(diffusion_model, num_samples, num_steps=200, device=device)\n",
    "show_samples(samples, n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a98b10-0643-4a63-a199-08ec9a3a0b88",
   "metadata": {},
   "source": [
    "Cannot run code above because kernel dies when I do. I attempted to change the batch size or the number of samples or number of epochs but seems to die everytime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af03f289-b812-4da5-8627-1ad6d72dd060",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.float64(np.load(\"MNIST_data.npy\"))\n",
    "data = data / 255\n",
    "data.shape\n",
    "\n",
    "samples = sample_images(diffusion_model, num_samples, num_steps=200, device=device)\n",
    "\n",
    "train_data=data[0:50000].reshape((-1,1,28,28))\n",
    "\n",
    "data_train = torch.tensor(train_data)\n",
    "\n",
    "real_features = data_train[:num_samples]\n",
    "generated_features = samples\n",
    "\n",
    "fid_score = calculate_fid(real_features, generated_features)\n",
    "print(f\"FID Score: {fid_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937228f3-06d7-4a01-bc7e-0c5a92760a04",
   "metadata": {},
   "source": [
    "Code: Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28103c7-3cb6-4918-a2fd-b73c1558cc99",
   "metadata": {},
   "source": [
    "2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04dd8bf1-a123-451a-b455-88911a64b73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Diffusion(nn.Module):\n",
    "    def __init__(self, model, n_steps, device, min_beta, max_beta):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.n_steps = n_steps\n",
    "        self.device = device\n",
    "        \n",
    "        #alpha = 1 - beta and alpha bar is product sum of alphas\n",
    "        self.beta = torch.linspace(min_beta, max_beta, n_steps).to(device)\n",
    "        self.alpha = 1 - self.beta\n",
    "        self.alpha_bar = torch.cumprod(self.alpha, dim = 0).to(device)\n",
    "        #store beta, alpha, \\bar alpha    \n",
    "    \n",
    "    def forward_process(self, x0, t):\n",
    "        \n",
    "        # finding xt given x0, sqrt of alpha bar times x0 + the sqrt of 1 - alpha bar epsilon\n",
    "\n",
    "        #compute Xj and epsilon j\n",
    "        noise = torch.randn_like(x0).to(self.device)\n",
    "        alpha_bar_t = self.alpha_bar[t].reshape(-1, 1, 1, 1)\n",
    "        x_t = torch.sqrt(alpha_bar_t) * x0 + torch.sqrt(1 - alpha_bar_t)* noise\n",
    "        return x_t, noise\n",
    "    #sample x_{t-1}, x_t, given x_0\n",
    "    \n",
    "    def predict_next(self, xt, t):\n",
    "        #use mu xt theta formula from number 8 in theory\n",
    "        t_tensor = (t / self.n_steps).float().view(xt.shape[0], 1).to(self.device)\n",
    "        e_theta_xt = self.model(xt, t_tensor)\n",
    "        alpha_t = self.alpha[t].reshape(-1, 1, 1, 1)\n",
    "        alpha_bar_t = self.alpha_bar[t].reshape(-1, 1, 1, 1)\n",
    "        \n",
    "        mu_xt_theta = (1 / torch.sqrt(alpha_t)) * (xt - ((1 - alpha_t) / torch.sqrt(1 - alpha_bar_t)) * e_theta_xt)\n",
    "        return mu_xt_theta\n",
    "    \n",
    "    #compute mu(xt, t)\n",
    "    \n",
    "    def compute_loss(self, x0):\n",
    "        # get batch size from x0\n",
    "        batch_size = x0.shape[0]\n",
    "        # get random time step t so we can train on different time steps\n",
    "        t = torch.randint(0, self.n_steps, (batch_size,), device = self.device).long()\n",
    "        \n",
    "        x_t_list = []\n",
    "        noise_list = []\n",
    "        # for each batch we sample xt and xt-1 given x0 at the sampled time step\n",
    "        for i in range(batch_size):\n",
    "          x_t, noise = self.forward_process(x0[i:i+1], t[i])\n",
    "          x_t_list.append(x_t)\n",
    "          noise_list.append(noise)\n",
    "        \n",
    "        x_t = torch.cat(x_t_list, dim = 0)\n",
    "        noise = torch.cat(noise_list, dim = 0)\n",
    "        # use model to predict ej based off of Xtj\n",
    "        e_theta_xt = self.model(x_t, (t / self.n_steps).float().view(-1,1).to(self.device))\n",
    "        #loss contribution then avg\n",
    "        loss = ((noise - e_theta_xt) ** 2).mean()\n",
    "        return loss\n",
    "    \n",
    "    def reverse(self, x_T, num_steps):\n",
    "        #iterate over time steps in reverse order, start from T-1 to 1\n",
    "        \n",
    "        x = x_T\n",
    "        for t in reversed(range(1, num_steps)):\n",
    "          # use predicted mean, gives estimated denoised version of the current sample\n",
    "          mu_xt_theta = self.predict_next(x, t)\n",
    "          # if t > 1 add gaussian noise to the sample, if t =1 then zero noise\n",
    "          noise = torch.randn_like(x).to(self.device) if t > 1 else torch.zeros_like(x).to(self.device)\n",
    "          # use beta to denoise sample\n",
    "          beta_t = self.beta[t].reshape(-1, 1, 1, 1)\n",
    "          #gets sample xt-1 using the predicted mean and scaling by Bt\n",
    "          x = mu_xt_theta + torch.sqrt(beta_t) * noise\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f58020-6969-455b-96d0-7f3594ab760d",
   "metadata": {},
   "source": [
    "2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb0b7f25-36f7-4883-ad62-e00f2a7aee8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_mnist():\n",
    "    data = np.float64(np.load(\"MNIST_data.npy\"))\n",
    "    labels = np.float32(np.load(\"MNIST_labels.npy\"))\n",
    "    \n",
    "    data = data / 255\n",
    "    data.shape\n",
    "    labels.shape\n",
    "    \n",
    "    print(data.shape)\n",
    "    data=np.float32(data)/255.\n",
    "    train_data=data[0:50000].reshape((-1,1,28,28))\n",
    "    train_labels=np.int32(labels[0:50000])\n",
    "    val_data=data[50000:55000].reshape((-1,1,28,28))\n",
    "    val_labels=np.int32(labels[55000:60000])\n",
    "    test_data=data[55000:65000].reshape((-1,1,28,28))\n",
    "    test_labels=np.int32(labels[55000:65000])\n",
    "    \n",
    "    data_train = torch.tensor(train_data)\n",
    "    labels_train = torch.tensor(train_labels, dtype=torch.long)\n",
    "    data_val = torch.tensor(val_data)\n",
    "    labels_val = torch.tensor(val_labels, dtype=torch.long)\n",
    "    data_test = torch.tensor(test_data)\n",
    "    labels_test = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "    train_dataset = TensorDataset(data_train, labels_train)\n",
    "    val_dataset = TensorDataset(data_val, labels_val)\n",
    "    test_dataset = TensorDataset(data_test, labels_test)\n",
    "    batch_size = 100\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b031877e-3615-4ddd-a7f9-f59ebe56c7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_diffusion(model, train_loader, val_loader, num_epochs, device):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch in train_loader:\n",
    "            x0, _ = batch\n",
    "            x0 = x0.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.compute_loss(x0)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /=len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                x0, _ = batch\n",
    "                x0 = x0.to(device)\n",
    "                loss = model.compute_loss(x0)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/ {num_epochs}, Train Loss: {train_loss: .4f}, Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca5fd378-8cb9-437d-baee-481e77b2f330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "Epoch 1/ 60, Train Loss:  0.3337, Val Loss: 0.0459\n",
      "Epoch 2/ 60, Train Loss:  0.0328, Val Loss: 0.0244\n",
      "Epoch 3/ 60, Train Loss:  0.0207, Val Loss: 0.0186\n",
      "Epoch 4/ 60, Train Loss:  0.0159, Val Loss: 0.0139\n",
      "Epoch 5/ 60, Train Loss:  0.0123, Val Loss: 0.0098\n",
      "Epoch 6/ 60, Train Loss:  0.0064, Val Loss: 0.0038\n",
      "Epoch 7/ 60, Train Loss:  0.0039, Val Loss: 0.0029\n",
      "Epoch 8/ 60, Train Loss:  0.0030, Val Loss: 0.0029\n",
      "Epoch 9/ 60, Train Loss:  0.0029, Val Loss: 0.0022\n",
      "Epoch 10/ 60, Train Loss:  0.0023, Val Loss: 0.0020\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m score_model \u001b[38;5;241m=\u001b[39m ScoreNet()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      5\u001b[0m diffusion_model \u001b[38;5;241m=\u001b[39m Diffusion(score_model, n_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice, min_beta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m, max_beta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtrain_diffusion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdiffusion_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 10\u001b[0m, in \u001b[0;36mtrain_diffusion\u001b[0;34m(model, train_loader, val_loader, num_epochs, device)\u001b[0m\n\u001b[1;32m      8\u001b[0m x0 \u001b[38;5;241m=\u001b[39m x0\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      9\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 10\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[3], line 54\u001b[0m, in \u001b[0;36mDiffusion.compute_loss\u001b[0;34m(self, x0)\u001b[0m\n\u001b[1;32m     52\u001b[0m noise \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(noise_list, dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# use model to predict ej based off of Xtj\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m e_theta_xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m#loss contribution then avg\u001b[39;00m\n\u001b[1;32m     56\u001b[0m loss \u001b[38;5;241m=\u001b[39m ((noise \u001b[38;5;241m-\u001b[39m e_theta_xt) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 86\u001b[0m, in \u001b[0;36mScoreNet.forward\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     84\u001b[0m h3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgnorm3(h3)\n\u001b[1;32m     85\u001b[0m h3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(h3)\n\u001b[0;32m---> 86\u001b[0m h4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh3\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m h4 \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense4(embed)\n\u001b[1;32m     88\u001b[0m h4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgnorm4(h4)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader, test_dataset = get_mnist()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "score_model = ScoreNet().to(device)\n",
    "diffusion_model = Diffusion(score_model, n_steps=200, device=device, min_beta=0.0001, max_beta=0.1).to(device)\n",
    "train_diffusion(diffusion_model, train_loader, val_loader, num_epochs=60, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1df1d0b-8e8c-4aca-acee-7a00d97cd7df",
   "metadata": {},
   "source": [
    "2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8042d37b-6a14-4733-ab6c-df6307383167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images(model, num_samples, num_steps, device):\n",
    "    x_T = torch.randn(num_samples, 1, 28, 28).to(device)\n",
    "    samples = model.reverse(x_T, num_steps)\n",
    "    return samples\n",
    "\n",
    "def show_samples(samples, n = 20):\n",
    "    samples = samples.cpu().detach().numpy()\n",
    "    fig, axes = plt.subplots(1, n, figsize = (n, 1))\n",
    "    for i in range(n):\n",
    "        axes[1].imshow(samples[i, 0], cmap = 'gray')\n",
    "        axes[i].axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803a4ad8-6812-4bb0-8dd1-f30175d31f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000\n",
    "show_samples(samples, n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d92fda7-bd54-4632-b1d1-ae268c5a2c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.float64(np.load(\"MNIST_data.npy\"))\n",
    "data = data / 255\n",
    "data.shape\n",
    "num_samples = 100\n",
    "samples = sample_images(diffusion_model, num_samples, num_steps=200, device=device)\n",
    "\n",
    "data=np.float32(data)/255.\n",
    "train_data=data[0:50000].reshape((-1,1,28,28))\n",
    "\n",
    "data_train = torch.tensor(train_data)\n",
    "\n",
    "real_features = data_train[:num_samples]\n",
    "generated_features = samples\n",
    "\n",
    "fid_score = calculate_fid(real_features, generated_features)\n",
    "print(f\"FID Score: {fid_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0715260-afa8-466b-8d57-667e888147a2",
   "metadata": {},
   "source": [
    "Can not run above because kills kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99861ea8-4aa5-4a31-861e-c45345a249d2",
   "metadata": {},
   "source": [
    "![Screen Shot 2024-05-23 at 11.12.41 AM.png](attachment:46c8c8a2-cb32-4b99-a403-c5b808fde734.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5c17e1-7311-4bcb-870d-b431a1209807",
   "metadata": {},
   "source": [
    "3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d46d03-2362-4b75-a4bd-043d216c3b45",
   "metadata": {},
   "source": [
    "When looking at the models we have created in parts 1 and 2 the main difference is the additional rho parameters in part 1 that are learned during training. Learning these parametrs can help handle scaling the noise component, and adding complexity and optimization. The loss function in part 2 will only focus on the squared error between the predicted and actual noise components. Additional parametrs can make models harder to train but makes them more adaptable in the process of running. We do see a difference in the image quality results and loss in part 1 and 2. The rho parameters in part 1 can help generate higher quality sample images. However, when looking at the loss function's results it can be easier to interpret the actual noise components values since there is no parameter offset. Since part 1 produces higher quality images due to its additional parameters, the FID score is higher for part 1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
