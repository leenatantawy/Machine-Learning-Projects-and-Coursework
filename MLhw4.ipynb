{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "363ebcb2-5eb4-4506-a640-4974713a254e",
   "metadata": {},
   "source": [
    "Question 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ea6560-9acd-464f-b208-d9e2d67557c2",
   "metadata": {},
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf359083-ec51-4ff2-aafe-d712dfe6413d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import binarize\n",
    "\n",
    "mnist_data = np.load(\"MNIST_data.npy\")\n",
    "mnist_labels = np.load(\"MNIST_labels.npy\")\n",
    "\n",
    "mnist_data = mnist_data/255\n",
    "mnist_data = binarize(mnist_data, threshold = 0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cff75e44-4232-4570-8124-23bb530d29e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_temp, labels_train, labels_temp = train_test_split(mnist_data, mnist_labels, test_size=0.4, random_state=42)\n",
    "data_dev, data_test, labels_dev, labels_test = train_test_split(data_temp, labels_temp, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ae880d6-3c0a-4147-b800-8c682823e521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def em_bernoulli_mixture(X, M, max_iter=100, tol=1e-4):\n",
    "    n_samples, n_features = X.shape\n",
    "    pis = np.random.rand(M, n_features)\n",
    "    weights = np.full(M, 1/M)\n",
    "    log_likelihood = 0\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        responsibilities = np.zeros((n_samples, M))\n",
    "        for j in range(M):\n",
    "            likelihood = np.prod(pis[j]**X * (1-pis[j])**(1-X), axis=1)\n",
    "            responsibilities[:, j] = weights[j] * likelihood\n",
    "\n",
    "        responsibilities /= (responsibilities.sum(axis=1, keepdims=True) + 1e-10)\n",
    "\n",
    "        effective_n = responsibilities.sum(axis=0)\n",
    "        weights = effective_n / n_samples\n",
    "        pis = (responsibilities.T @ X) / (effective_n[:, np.newaxis] + 1e-10)\n",
    "\n",
    "        new_log_likelihood = np.sum(np.log(np.maximum(responsibilities.sum(axis=1), 1e-10)))\n",
    "        if np.abs(new_log_likelihood - log_likelihood) < tol:\n",
    "            break\n",
    "        log_likelihood = new_log_likelihood\n",
    "\n",
    "    return pis, weights, log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03427a7f-ef75-41c3-9cd3-f5c77fd46c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of datasets:\n",
      "X_train: (42000, 784)\n",
      "X_dev: (14000, 784)\n",
      "X_test: (14000, 784)\n",
      "y_train: (42000,)\n",
      "y_dev: (14000,)\n",
      "y_test: (14000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shapes of datasets:\")\n",
    "print(\"X_train:\", data_train.shape)\n",
    "print(\"X_dev:\", data_dev.shape)\n",
    "print(\"X_test:\", data_test.shape)\n",
    "print(\"y_train:\", labels_train.shape)\n",
    "print(\"y_dev:\", labels_dev.shape)\n",
    "print(\"y_test:\", labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50a587b6-2046-4299-b925-21a477ce7de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bernoulli(data, labels, M):\n",
    "    unique_labels = np.unique(labels)\n",
    "    n_features = data.shape[1]\n",
    "    models = {c: [] for c in unique_labels}\n",
    "\n",
    "    for c in unique_labels:\n",
    "        data_c = data[labels == c]\n",
    "        if M == 1:\n",
    "            p = np.mean(data_c, axis = 0)\n",
    "            models[c].append(p)\n",
    "        else:\n",
    "            for m in range(M):\n",
    "                p = np.random.rand(n_features)\n",
    "                models[c].append(p)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36f178e8-5cd6-481e-9f98-1f180e9d5be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "M_vals = [1,5,10,20]\n",
    "class_models = {}\n",
    "\n",
    "for M in M_vals:\n",
    "    models = bernoulli(data_train, labels_train, M)\n",
    "    class_models[M] = models\n",
    "\n",
    "data_combined = np.concatenate((data_train, data_dev))\n",
    "labels_combines = np.concatenate((labels_train, labels_dev))\n",
    "optimal_M = 5\n",
    "final_models = bernoulli(data_combined, labels_combines, optimal_M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5508db55-7941-4c6a-9528-fed7c463ee75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66a18f55-637c-45f9-861c-0bcad80be148",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_bernoulli(x, p):\n",
    "    return x * np.log(p+ 1e-9) + (1-x) * np.log(1-p + 1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd1413ed-7cef-49e6-9b66-8e0e13022178",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(data, models):\n",
    "    predictions = []\n",
    "    for i in range(data.shape[0]):\n",
    "        max_prob = -np.inf\n",
    "        pred_class = None\n",
    "        for c in models:\n",
    "            prob_c = np.log(1/10)\n",
    "            for p in models[c]:\n",
    "                prob_c += np.sum(log_bernoulli(data[i], p))\n",
    "            if prob_c > max_prob:\n",
    "                max_prob = prob_c\n",
    "                pred_class = c\n",
    "        predictions.append(pred_class)\n",
    "    return np.array(predictions)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad8de111-f448-4a96-94ba-dcf5010692bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_calc(predictions, labels):\n",
    "    return np.mean(predictions == labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5259daa-5602-4f51-82b5-818ce679be9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.8364285714285714\n"
     ]
    }
   ],
   "source": [
    "M_vals = [1,5,10,20]\n",
    "class_models = {}\n",
    "\n",
    "for M in M_vals:\n",
    "    models = bernoulli(data_train, labels_train, M)\n",
    "    class_models[M] = models\n",
    "\n",
    "data_combined = np.concatenate((data_train, data_dev))\n",
    "labels_combines = np.concatenate((labels_train, labels_dev))\n",
    "optimal_M = 1\n",
    "final_models = bernoulli(data_combined, labels_combines, optimal_M)\n",
    "\n",
    "predictions1= classify(data_test, final_models)\n",
    "accuracy = np.mean(predictions1 == labels_test)\n",
    "print(\"Test set accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "752d3212-8e79-402e-bc67-7c25eaf8f1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.10721428571428572\n"
     ]
    }
   ],
   "source": [
    "M_vals = [1,5,10,20]\n",
    "class_models = {}\n",
    "\n",
    "for M in M_vals:\n",
    "    models = bernoulli(data_train, labels_train, M)\n",
    "    class_models[M] = models\n",
    "\n",
    "data_combined = np.concatenate((data_train, data_dev))\n",
    "labels_combines = np.concatenate((labels_train, labels_dev))\n",
    "optimal_M = 5\n",
    "final_models = bernoulli(data_combined, labels_combines, optimal_M)\n",
    "\n",
    "predictions5= classify(data_test, final_models)\n",
    "accuracy = np.mean(predictions5 == labels_test)\n",
    "print(\"Test set accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db98bee0-ca23-499e-b599-cbfc7845a4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.09342857142857143\n"
     ]
    }
   ],
   "source": [
    "M_vals = [1,5,10,20]\n",
    "class_models = {}\n",
    "\n",
    "for M in M_vals:\n",
    "    models = bernoulli(data_train, labels_train, M)\n",
    "    class_models[M] = models\n",
    "\n",
    "data_combined = np.concatenate((data_train, data_dev))\n",
    "labels_combines = np.concatenate((labels_train, labels_dev))\n",
    "optimal_M = 10\n",
    "final_models = bernoulli(data_combined, labels_combines, optimal_M)\n",
    "\n",
    "predictions10= classify(data_test, final_models)\n",
    "accuracy = np.mean(predictions10 == labels_test)\n",
    "print(\"Test set accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "723fa5df-7c5a-4227-9448-0b5d923a9640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.11078571428571428\n"
     ]
    }
   ],
   "source": [
    "M_vals = [1,5,10,20]\n",
    "class_models = {}\n",
    "\n",
    "for M in M_vals:\n",
    "    models = bernoulli(data_train, labels_train, M)\n",
    "    class_models[M] = models\n",
    "\n",
    "data_combined = np.concatenate((data_train, data_dev))\n",
    "labels_combines = np.concatenate((labels_train, labels_dev))\n",
    "optimal_M = 20\n",
    "final_models = bernoulli(data_combined, labels_combines, optimal_M)\n",
    "\n",
    "predictions20 = classify(data_test, final_models)\n",
    "accuracy = np.mean(predictions20 == labels_test)\n",
    "print(\"Test set accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dc02d6-d611-44c4-a7f4-377dd3b08ccf",
   "metadata": {},
   "source": [
    "The optimal M is M = 10 as it has the lowest test set error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "061d5d2a-bae9-40e6-974b-35ce1934d258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy for M = 5: 0.9007857142857143\n",
      "Test accuracy for M = 10: 0.9007857142857143\n",
      "Test accuracy for M = 20: 0.9007857142857143\n"
     ]
    }
   ],
   "source": [
    "M_vals = [5,10,20]\n",
    "\n",
    "results = {}\n",
    "for m in M_vals:\n",
    "    models = {}\n",
    "    for label in np.unique(labels_train):\n",
    "        data_class = data_train[labels_train == label]\n",
    "        pis, weights, _ = em_bernoulli_mixture(data_class, m)\n",
    "        models[label] = pis\n",
    "        \n",
    "    predictions = classify(data_test, models)\n",
    "    error = error_calc(predictions, labels_test)\n",
    "    results[m] = error\n",
    "\n",
    "for M, err in results.items():\n",
    "    print(f'Test accuracy for M = {M}: {err}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3321c13e-429a-4254-9ebc-980d1a5b8f6b",
   "metadata": {},
   "source": [
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3cffed6e-2e98-40eb-aabe-19527b69d93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "c_vals = [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]\n",
    "best_error_rate = float('inf')\n",
    "best_c = None\n",
    "\n",
    "for c in c_vals:\n",
    "    log_reg = LogisticRegression(fit_intercept=True, C = c, penalty = 'l2', multi_class = 'multinomial', solver = 'lbfgs', max_iter = 1000)\n",
    "    log_reg.fit(data_train, labels_train)\n",
    "\n",
    "    labels_predicted_dev = log_reg.predict(data_dev)\n",
    "    correct_predictions_dev = np.sum(labels_predicted_dev == labels_dev)\n",
    "    error_rate_dev = 1- (correct_predictions_dev / labels_dev.size)\n",
    "\n",
    "    if error_rate_dev < best_error_rate:\n",
    "        best_error_rate = error_rate_dev\n",
    "        best_c = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5e829942-82b4-4bea-bbe8-63a80f6f5c89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08221428571428568"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg_optimal = LogisticRegression(fit_intercept = True, C = best_c, penalty = 'l2', multi_class = 'multinomial', solver = 'lbfgs', max_iter = 1000)\n",
    "log_reg_optimal.fit(data_combined, labels_combines)\n",
    "\n",
    "labels_pred_test = log_reg_optimal.predict(data_test)\n",
    "correct_pred_test = np.sum(labels_pred_test == labels_test)\n",
    "error_rate_test = 1 - (correct_pred_test / labels_test.size)\n",
    "error_rate_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff42869-eb82-40c5-8942-f61baec3fdf0",
   "metadata": {},
   "source": [
    "The logistic regression had a slightly higher error rate than the best M using bernoulli but was faster in run time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "87ba0fdf-200a-411a-9357-c95e55d433aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n"
     ]
    }
   ],
   "source": [
    "print(best_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0698f215-3f8a-40a2-880b-a3988a406af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_center = mnist_data - np.mean(mnist_data)\n",
    "\n",
    "covariance_mtx = np.cov(data_center.T)\n",
    "eigenvalues, eigenvectors = np.linalg.eigh(covariance_mtx)\n",
    "\n",
    "sorted_i = np.argsort(eigenvalues)[::-1]\n",
    "sorted_vals = eigenvalues[sorted_i]\n",
    "sorted_vecs = eigenvectors[:, sorted_i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3f48c974-e035-41c2-b5dd-ffac403ff86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def project(data, pcs):\n",
    "    return np.dot(data, pcs)\n",
    "\n",
    "k_vals = [10, 20, 50, 100, 200, 300]\n",
    "c_vals = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "\n",
    "best_error_rate = float('inf')\n",
    "best_k = None\n",
    "best_c = None\n",
    "\n",
    "for k in k_vals:\n",
    "    principal_components = sorted_vecs[:, :k]\n",
    "    data_train_pca = project(data_train - np.mean(mnist_data, axis=0), principal_components)\n",
    "    data_dev_pca = project(data_dev - np.mean(mnist_data, axis = 0), principal_components)\n",
    "\n",
    "    for c in c_vals:\n",
    "        log_reg_pca = LogisticRegression(fit_intercept = True, C=c, penalty = 'l2', multi_class = 'multinomial', solver = 'lbfgs', max_iter = 1000)\n",
    "        log_reg_pca.fit(data_train_pca, labels_train)\n",
    "\n",
    "        labels_pred_dev_pca = log_reg_pca.predict(data_dev_pca)\n",
    "        correct_predictions_dev = np.sum(labels_pred_dev_pca == labels_dev)\n",
    "        error_rate_dev = 1 - (correct_predictions_dev /labels_dev.size)\n",
    "\n",
    "        if error_rate_dev < best_error_rate:\n",
    "            best_error_rate = error_rate_dev\n",
    "            best_k = k\n",
    "            best_c = c\n",
    "\n",
    "\n",
    "data_combined_pca = np.dot(data_combined, sorted_vecs[:, :best_k])\n",
    "log_reg_final = LogisticRegression(fit_intercept = True, C = best_c, penalty = 'l2', multi_class = 'multinomial', solver = 'lbfgs', max_iter = 1000)\n",
    "log_reg_final.fit(data_combined_pca, labels_combines)\n",
    "\n",
    "data_test_pca = np.dot(data_test - np.mean(mnist_data, axis = 0), sorted_vecs[:, :best_k])\n",
    "labels_pred_test_pca = log_reg_final.predict(data_test_pca)\n",
    "correct_predictions_test = np.sum(labels_pred_test_pca == labels_test)\n",
    "error_rate_test_pca = 1 - (correct_predictions_test / labels_test.size)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd1b8ae4-0765-4b25-ae0c-69424b6ff14d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a16eaf18-a456-42b0-b679-011b24039a41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cfc4a598-d601-43b2-922c-fac974c091e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23985714285714288"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_rate_test_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03335dde-3d48-44d3-a508-1dc87814f640",
   "metadata": {},
   "source": [
    "The logistic regression using pca had a higher error rate than the previous logistic regression. This could be because with PCA some values are omitted as we use the most relevant eigenvalues. However with pca the run time was faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7c4afaa8-33f7-4823-bdf3-70823111ca0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training NLL: -1128.0685545274123, Validation NLL: -1135.3504190761173\n",
      "Epoch 2, Training NLL: -1712.7912016736295, Validation NLL: -1725.3096590091018\n",
      "Epoch 3, Training NLL: -2015.533791276489, Validation NLL: -2031.8201906286245\n",
      "Epoch 4, Training NLL: -2172.5017538899083, Validation NLL: -2191.5103033908235\n",
      "Epoch 5, Training NLL: -2253.578576151778, Validation NLL: -2274.5345441837662\n",
      "Epoch 6, Training NLL: -2295.0482556751504, Validation NLL: -2317.385731852646\n",
      "Epoch 7, Training NLL: -2316.913535395464, Validation NLL: -2340.2652474741967\n",
      "Epoch 8, Training NLL: -2328.072869686831, Validation NLL: -2352.142326435105\n",
      "Epoch 9, Training NLL: -2333.8116784441877, Validation NLL: -2358.3960516519187\n",
      "Epoch 10, Training NLL: -2336.8332414604906, Validation NLL: -2361.7786182743544\n",
      "Epoch 11, Training NLL: -2338.490124390355, Validation NLL: -2363.709265088958\n",
      "Epoch 12, Training NLL: -2339.2659717390693, Validation NLL: -2364.6735547232706\n",
      "Epoch 13, Training NLL: -2339.70636319715, Validation NLL: -2365.263159132917\n",
      "Epoch 14, Training NLL: -2339.933911514147, Validation NLL: -2365.601401806249\n",
      "Epoch 15, Training NLL: -2340.0447407288157, Validation NLL: -2365.784620206583\n",
      "Epoch 16, Training NLL: -2340.1125585402574, Validation NLL: -2365.908226565139\n",
      "Epoch 17, Training NLL: -2340.143244657137, Validation NLL: -2365.9761290351234\n",
      "Epoch 18, Training NLL: -2340.1563961538272, Validation NLL: -2366.014538811994\n",
      "Epoch 19, Training NLL: -2340.1636046817057, Validation NLL: -2366.033199033328\n",
      "Epoch 20, Training NLL: -2340.1671880020344, Validation NLL: -2366.050731697081\n",
      "Epoch 21, Training NLL: -2340.1710122859126, Validation NLL: -2366.0674250936095\n",
      "Epoch 22, Training NLL: -2340.171378859282, Validation NLL: -2366.077092885486\n",
      "Epoch 23, Training NLL: -2340.17363856735, Validation NLL: -2366.0843259766857\n",
      "Epoch 24, Training NLL: -2340.173788793358, Validation NLL: -2366.087477025159\n",
      "Epoch 25, Training NLL: -2340.173667023148, Validation NLL: -2366.08294947483\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from scipy.special import softmax, logsumexp\n",
    "\n",
    "def sgd_logistic_regression(X_train, y_train, X_val, y_val, batch_size, learning_rate, lambda_reg, epochs):\n",
    "    theta = np.zeros((X_train.shape[1],))\n",
    "    \n",
    "    def negative_log_likelihood(X, y, theta):\n",
    "        logits = X.dot(theta)\n",
    "        log_likelihoods = -logsumexp(np.c_[logits, np.zeros_like(logits)], axis=1) + logits * y\n",
    "        return -np.sum(log_likelihoods) / len(y) + (lambda_reg / 2) * np.sum(theta ** 2)\n",
    "\n",
    "    def gradient(X, y, theta):\n",
    "        logits = X.dot(theta)\n",
    "        predictions = softmax(logits[:, np.newaxis], axis=1)[:, 0]\n",
    "        gradient = -X.T.dot(y - predictions) / len(y) + lambda_reg * theta\n",
    "        return gradient\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        permutation = np.random.permutation(len(X_train))\n",
    "        X_train_shuffled = X_train[permutation]\n",
    "        y_train_shuffled = y_train[permutation]\n",
    "        for i in range(0, len(X_train), batch_size):\n",
    "            X_batch = X_train_shuffled[i:i + batch_size]\n",
    "            y_batch = y_train_shuffled[i:i + batch_size]\n",
    "            theta -= learning_rate * gradient(X_batch, y_batch, theta)\n",
    "\n",
    "        train_nll = negative_log_likelihood(X_train, y_train, theta)\n",
    "        val_nll = negative_log_likelihood(X_val, y_val, theta)\n",
    "        print(f'Epoch {epoch+1}, Training NLL: {train_nll}, Validation NLL: {val_nll}')\n",
    "        \n",
    "        if epoch > 0 and val_nll >= previous_val_nll:\n",
    "            break\n",
    "        previous_val_nll = val_nll\n",
    "    \n",
    "    return theta\n",
    "\n",
    "batch_size = 128\n",
    "learning_rate = 0.01\n",
    "lambda_reg = 0.1\n",
    "epochs = 100\n",
    "\n",
    "\n",
    "theta = sgd_logistic_regression(data_train, labels_train, data_dev, labels_dev, batch_size, learning_rate, lambda_reg, epochs)\n",
    "\n",
    "def predict(X, theta):\n",
    "    theta = np.atleast_2d(theta).T\n",
    "\n",
    "    \n",
    "    logits = X.dot(theta)\n",
    "    probs = softmax(logits, axis = 1)\n",
    "\n",
    "    if probs.shape[1] == 1:\n",
    "        return (probs>0.5).astype(int).flatten()\n",
    "\n",
    "    return np.argmax(probs, axis = 1)\n",
    "\n",
    "\n",
    "labels_pred_test = predict(data_test, theta)\n",
    "error_rate_test = np.mean(labels_pred_test != labels_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "58412fbd-b39a-4b04-8b6a-d583f04038e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with λ = 0\n",
      "Epoch 1, Training NLL: -3075.331249959077, Validation NLL: -3092.3688277232136\n",
      "Epoch 2, Training NLL: -6151.149952161459, Validation NLL: -6185.232746015626\n",
      "Epoch 3, Training NLL: -9226.588910669641, Validation NLL: -9277.713071484373\n",
      "Epoch 4, Training NLL: -12300.58506292783, Validation NLL: -12368.741957511162\n",
      "Epoch 5, Training NLL: -15375.318328333333, Validation NLL: -15460.509217968747\n",
      "Epoch 6, Training NLL: -18452.295416249995, Validation NLL: -18554.535917477675\n",
      "Epoch 7, Training NLL: -21527.426509289435, Validation NLL: -21646.70589770089\n",
      "Epoch 8, Training NLL: -24602.804683333325, Validation NLL: -24739.122412700886\n",
      "Epoch 9, Training NLL: -27676.90410908481, Validation NLL: -27830.25147388392\n",
      "Epoch 10, Training NLL: -30751.163972135408, Validation NLL: -30921.546928459808\n",
      "Epoch 11, Training NLL: -33826.61653135787, Validation NLL: -34014.03779407364\n",
      "Epoch 12, Training NLL: -36900.79816720981, Validation NLL: -37105.24934845981\n",
      "Epoch 13, Training NLL: -39976.35178464656, Validation NLL: -40197.845558404006\n",
      "Epoch 14, Training NLL: -43051.42457684894, Validation NLL: -43289.9526091741\n",
      "Epoch 15, Training NLL: -46126.56280446426, Validation NLL: -46382.13318863836\n",
      "Epoch 16, Training NLL: -49200.17411743671, Validation NLL: -49472.771766261125\n",
      "Epoch 17, Training NLL: -52276.964475342225, Validation NLL: -52566.614098794606\n",
      "Epoch 18, Training NLL: -55353.84982831469, Validation NLL: -55660.53945944192\n",
      "Epoch 19, Training NLL: -58428.618715349665, Validation NLL: -58752.351058694156\n",
      "Epoch 20, Training NLL: -61502.69863379089, Validation NLL: -61843.459599207556\n",
      "Epoch 21, Training NLL: -64578.199352377196, Validation NLL: -64936.00025933032\n",
      "Epoch 22, Training NLL: -67652.65266009296, Validation NLL: -68027.48510651781\n",
      "Epoch 23, Training NLL: -70728.90089178192, Validation NLL: -71120.77508772316\n",
      "Epoch 24, Training NLL: -73803.20255952002, Validation NLL: -74212.11583908476\n",
      "Epoch 25, Training NLL: -76876.20267536081, Validation NLL: -77302.14826329233\n",
      "Epoch 26, Training NLL: -79950.64854980649, Validation NLL: -80393.62487052451\n",
      "Epoch 27, Training NLL: -83025.65151937495, Validation NLL: -83485.6610350111\n",
      "Epoch 28, Training NLL: -86100.0431336532, Validation NLL: -86577.08621236602\n",
      "Epoch 29, Training NLL: -89175.84068327746, Validation NLL: -89669.93128068074\n",
      "Epoch 30, Training NLL: -92250.08499266734, Validation NLL: -92761.2087722544\n",
      "Epoch 31, Training NLL: -95326.56799847462, Validation NLL: -95854.73566543518\n",
      "Epoch 32, Training NLL: -98401.36375263013, Validation NLL: -98946.57132786822\n",
      "Epoch 33, Training NLL: -101475.68494485482, Validation NLL: -102037.92335186375\n",
      "Epoch 34, Training NLL: -104550.38075980275, Validation NLL: -105129.65523970974\n",
      "Epoch 35, Training NLL: -107625.78071862343, Validation NLL: -108222.08969035707\n",
      "Epoch 36, Training NLL: -110700.74973084813, Validation NLL: -111314.09121371644\n",
      "Epoch 37, Training NLL: -113775.21401560634, Validation NLL: -114405.58605113832\n",
      "Epoch 38, Training NLL: -116850.62536679306, Validation NLL: -117498.0384611718\n",
      "Epoch 39, Training NLL: -119926.44736904012, Validation NLL: -120590.90850468742\n",
      "Epoch 40, Training NLL: -123002.22185132063, Validation NLL: -123683.71394879457\n",
      "Epoch 41, Training NLL: -126077.43327625738, Validation NLL: -126775.96552593743\n",
      "Epoch 42, Training NLL: -129152.27117193073, Validation NLL: -129867.83690069188\n",
      "Epoch 43, Training NLL: -132227.6551828273, Validation NLL: -132960.2599936941\n",
      "Epoch 44, Training NLL: -135301.80786188235, Validation NLL: -136051.44622396195\n",
      "Epoch 45, Training NLL: -138376.1354955133, Validation NLL: -139142.80478993294\n",
      "Epoch 46, Training NLL: -141450.50705916656, Validation NLL: -142234.212347712\n",
      "Epoch 47, Training NLL: -144525.80357087785, Validation NLL: -145326.54870123873\n",
      "Epoch 48, Training NLL: -147601.4520705914, Validation NLL: -148419.23236941954\n",
      "Epoch 49, Training NLL: -150677.2893959969, Validation NLL: -151512.1006430021\n",
      "Epoch 50, Training NLL: -153752.22375535328, Validation NLL: -154604.075981886\n",
      "Epoch 51, Training NLL: -156827.54099540162, Validation NLL: -157696.43443831458\n",
      "Epoch 52, Training NLL: -159902.94001545003, Validation NLL: -160788.87211781237\n",
      "Epoch 53, Training NLL: -162975.95989411446, Validation NLL: -163878.9157406249\n",
      "Epoch 54, Training NLL: -166050.36615577, Validation NLL: -166970.35179659587\n",
      "Epoch 55, Training NLL: -169125.7409639657, Validation NLL: -170062.76312062496\n",
      "Epoch 56, Training NLL: -172201.70171112346, Validation NLL: -173155.77030790172\n",
      "Epoch 57, Training NLL: -175276.5854209263, Validation NLL: -176247.68831316958\n",
      "Epoch 58, Training NLL: -178353.16409244787, Validation NLL: -179341.3139741852\n",
      "Epoch 59, Training NLL: -181429.03807875738, Validation NLL: -182434.2337889397\n",
      "Epoch 60, Training NLL: -184502.94171744416, Validation NLL: -185525.16735522318\n",
      "Epoch 61, Training NLL: -187577.8668559747, Validation NLL: -188617.13153521204\n",
      "Epoch 62, Training NLL: -190653.60952727307, Validation NLL: -191709.91784869423\n",
      "Epoch 63, Training NLL: -193729.02648610485, Validation NLL: -194802.3728858705\n",
      "Epoch 64, Training NLL: -196803.03047332956, Validation NLL: -197893.40636611605\n",
      "Epoch 65, Training NLL: -199879.71637844486, Validation NLL: -200987.13512608255\n",
      "Epoch 66, Training NLL: -202954.45997294635, Validation NLL: -204078.91348744414\n",
      "Epoch 67, Training NLL: -206029.78303228048, Validation NLL: -207171.27245497765\n",
      "Epoch 68, Training NLL: -209105.48223606765, Validation NLL: -210264.01321620532\n",
      "Epoch 69, Training NLL: -212179.3919104687, Validation NLL: -213354.9538924888\n",
      "Epoch 70, Training NLL: -215255.30841682284, Validation NLL: -216447.9099517968\n",
      "Epoch 71, Training NLL: -218331.03178516365, Validation NLL: -219540.66812517852\n",
      "Epoch 72, Training NLL: -221406.56196273427, Validation NLL: -222633.2331124218\n",
      "Epoch 73, Training NLL: -224481.91438039424, Validation NLL: -225725.6193182477\n",
      "Epoch 74, Training NLL: -227556.06758847836, Validation NLL: -228816.81155578117\n",
      "Epoch 75, Training NLL: -230630.36829378342, Validation NLL: -231908.14500328118\n",
      "Epoch 76, Training NLL: -233707.87205685634, Validation NLL: -235002.69051066958\n",
      "Epoch 77, Training NLL: -236782.5839159337, Validation NLL: -238094.43863936377\n",
      "Epoch 78, Training NLL: -239856.96235039047, Validation NLL: -241185.85083247756\n",
      "Epoch 79, Training NLL: -242932.3588556621, Validation NLL: -244278.28427183029\n",
      "Epoch 80, Training NLL: -246005.4767358556, Validation NLL: -247368.42780036823\n",
      "Epoch 81, Training NLL: -249080.9328160825, Validation NLL: -250460.91383551332\n",
      "Epoch 82, Training NLL: -252156.25774489206, Validation NLL: -253553.27903698655\n",
      "Epoch 83, Training NLL: -255230.3046575408, Validation NLL: -256644.35260801335\n",
      "Epoch 84, Training NLL: -258305.61138168516, Validation NLL: -259736.70471518967\n",
      "Epoch 85, Training NLL: -261382.3318824478, Validation NLL: -262830.47440279013\n",
      "Epoch 86, Training NLL: -264456.38722314354, Validation NLL: -265921.5606210267\n",
      "Epoch 87, Training NLL: -267530.1944125669, Validation NLL: -269012.4007027901\n",
      "Epoch 88, Training NLL: -270605.5792720126, Validation NLL: -272104.8248328013\n",
      "Epoch 89, Training NLL: -273680.64774529764, Validation NLL: -275196.93357140623\n",
      "Epoch 90, Training NLL: -276755.8178111495, Validation NLL: -278289.1482832701\n",
      "Epoch 91, Training NLL: -279829.2230739322, Validation NLL: -281379.58026572544\n",
      "Epoch 92, Training NLL: -282903.3975643043, Validation NLL: -284470.78095046873\n",
      "Epoch 93, Training NLL: -285977.3568032478, Validation NLL: -287561.77336369426\n",
      "Epoch 94, Training NLL: -289051.72147725825, Validation NLL: -290653.1700772434\n",
      "Epoch 95, Training NLL: -292127.3709827865, Validation NLL: -293745.8518507032\n",
      "Epoch 96, Training NLL: -295203.18655484373, Validation NLL: -296838.70618228795\n",
      "Epoch 97, Training NLL: -298277.636369256, Validation NLL: -299930.1878432143\n",
      "Epoch 98, Training NLL: -301353.6886177976, Validation NLL: -303023.27679150674\n",
      "Epoch 99, Training NLL: -304427.54485425225, Validation NLL: -306114.1629893974\n",
      "Epoch 100, Training NLL: -307503.23001213535, Validation NLL: -309206.88572082587\n",
      "Training with λ = 10\n",
      "Epoch 1, Training NLL: -23.304302175261196, Validation NLL: -23.54558760829423\n",
      "Epoch 2, Training NLL: -23.325371939966942, Validation NLL: -23.594502762226334\n",
      "Epoch 3, Training NLL: -23.26545569386074, Validation NLL: -23.544876958841282\n",
      "Training with λ = 100\n",
      "Epoch 1, Training NLL: -1.699229462775247, Validation NLL: -1.7248661348617467\n",
      "Epoch 2, Training NLL: -1.4694473351453134, Validation NLL: -1.5004633977522492\n",
      "Test error for best λ (0): 0.8864\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "def train_and_evaluate_sgd(X_train, y_train, X_val, y_val, X_test, y_test, lambda_vals, learning_rate, epochs, batch_size):\n",
    "    best_lambda = None\n",
    "    best_validation_error = float(\"inf\")\n",
    "    training_errors = []\n",
    "    validation_errors = []\n",
    "    test_error = None\n",
    "\n",
    "    for lambda_reg in lambda_vals:\n",
    "        print(f\"Training with λ = {lambda_reg}\")\n",
    "        start_time = time.time()\n",
    "        theta = sgd_logistic_regression(X_train, y_train, X_val, y_val, batch_size, learning_rate, lambda_reg, epochs)\n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        labels_pred_val = predict(X_val, theta)\n",
    "        validation_error = np.mean(labels_pred_val != y_val)\n",
    "\n",
    "        if validation_error < best_validation_error:\n",
    "            best_validation_error = validation_error\n",
    "            best_lambda = lambda_reg\n",
    "            test_error = np.mean(predict(X_test, theta) != y_test)\n",
    "        \n",
    "    return best_lambda, test_error\n",
    "\n",
    "lambda_vals = [0, 10, 100]\n",
    "\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "\n",
    "best_lambda, test_error = train_and_evaluate_sgd(data_train, labels_train, data_dev, labels_dev, data_test, labels_test, lambda_vals, learning_rate, epochs, batch_size)\n",
    "\n",
    "print(f\"Test error for best λ ({best_lambda}): {test_error:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f64f12a2-5474-4d63-a35b-9da379270aa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8864285714285715"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_rate_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63f6f84-50b7-4b93-91ff-6c196674dcaf",
   "metadata": {},
   "source": [
    "The best error rate achieved seems to be with the Bernoulli mixtures. The Stochastic gradient descent was faster than the logistic regression but had a higher error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528674b7-7b73-4aaf-90ad-463ca152a02e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
